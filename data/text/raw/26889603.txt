
 properties manuscript? 
 
 
 0100725 
 3187 
 Cortex 
 Cortex 
 
 Cortex; a journal devoted to the study of the nervous system and behavior 
 
 0010-9452 
 1973-8102 
 
 
 26889603 
 5357080 
 10.1016/j.cortex.2016.01.002 
 NIHMS844239 
 
 
 Article 
 
 
 
 Mental imagery of speech implicates two mechanisms of perceptual reactivation 
 
 
 
 
 Tian 
 Xing 
 
 a 
 b 
 c 
 * 
 
 
 
 Zarate 
 Jean Mary 
 
 c 
 
 
 
 Poeppel 
 David 
 
 c 
 d 
 
 
 a New York University Shanghai, China 
 b NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai, China 
 c Department of Psychology, New York University, USA 
 d Department of Neuroscience, Max Planck Institute for Empirical Aesthetics (MPIEA), Germany 
 
 * Corresponding author. New York University Shanghai 1555 Century Avenue, Room 1138 Pudong, Shanghai 200122, China.  xing.tian@nyu.edu  (X. Tian) 
 
 
 8 
 2 
 2017 
 
 
 14 
 1 
 2016 
 
 
 4 
 2016 
 
 
 17 
 3 
 2017 
 
 77 
 1 
 12 
 
 
 This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law. 
 
 
 
 Sensory cortices can be activated without any external stimuli. Yet, it is still unclear how this perceptual reactivation occurs and which neural structures mediate this reconstruction process. In this study, we employed fMRI with mental imagery paradigms to investigate the neural networks involved in perceptual reactivation. Subjects performed two speech imagery tasks: articulation imagery (AI) and hearing imagery (HI). We found that AI induced greater activity in frontal-parietal sensorimotor systems, including sensorimotor cortex, subcentral (BA 43), middle frontal cortex (BA 46) and parietal operculum (PO), whereas HI showed stronger activation in regions that have been implicated in memory retrieval: middle frontal (BA 8), inferior parietal cortex and intraparietal sulcus. Moreover, posterior superior temporal sulcus (pSTS) and anterior superior temporal gyrus (aSTG) was activated more in AI compared with HI, suggesting that covert motor processes induced stronger perceptual reactivation in the auditory cortices. These results suggest that motor-to-perceptual transformation and memory retrieval act as two complementary mechanisms to internally reconstruct corresponding perceptual outcomes. These two mechanisms can serve as a neurocomputational foundation for predicting perceptual changes, either via a previously learned relationship between actions and their perceptual consequences or via stored perceptual experiences of stimulus and episodic or contextual regularity. 
 
 
 Prediction 
 Mental simulation 
 Sensorimotor integration 
 Internal forward model/efference 
 copy/corollary discharge 
 Memory retrieval 
 
 
 
 
 1. Introduction 
 Sensory cortices can be activated without any external stimulation (e.g.,  Ji & Wilson, 2006 ;  Wheeler, Petersen, & Buckner, 2000 ). That is, perceptual neural representations can be reconstructed without perceptual processing (referred to as perceptual reactivation). Mental imagery, defined as an internally generated quasi-perceptual experience, is one such example (e.g.,  Kosslyn et al., 1999 ;  Kraemer, Macrae, Green, & Kelley, 2005 ). The ability to form mental images has been hypothesized as a vehicle for generating and representing thoughts. This argument can be found as early as Plato’s Theaetetus [427–347 BC] ( 1987 ) and Aristotle’s De Anima [384–322 BC] ( 1986 ). In the age of enlightenment, mental imagery was considered analogous to perception by philosophers such as  Descartes (1642/1984) ,  Hobbes (1651/1968) ,  Berkeley (1734/1965a ,  1734/1965b)  and  Hume (1969) . Early experimental psychologists such as  Wundt (1913)  and  James (1890)  proposed that ideas were represented as mental images in both visual and auditory domains. Modern research in mental imagery has yielded insight on how thought is represented in cognitive systems ( Kosslyn, 1994 ;  Kosslyn, Ganis, & Thompson, 2001 ;  Paivio, 1971 ,  1986 ;  Pylyshyn, 1981 ,  2003 ). 
 Recently, an additional computational role of mental imagery has been proposed: a mechanism to plan possible future contingencies. That is, mental imagery has been modeled as a process in which perceptual consequences can be predicted to gain advantages in various aspects of perception, memory, decision making and motor control ( Albright, 2012 ;  Moulton & Kosslyn, 2009 ;  Schacter et al., 2012 ;  Tian & Poeppel, 2012 ). The reactivation of perceptual neural representations without any external stimulation is the key mechanism mediating this predictive ability ( Moulton & Kosslyn, 2009 ). Internally induced neural representations, which are highly similar to the ones established in corresponding perceptual processing, have been observed in modality-specific areas, such as in visual (e.g.,  Kosslyn et al., 1999 ;  O’Craven & Kanwisher, 2000 ), auditory (e.g.,  Kraemer et al., 2005 ;  Shergill et al., 2001 ;  Zatorre, Halpern, Perry, Meyer, & Evans, 1996 ), somatosensory (e.g.,  Yoo, Freeman, McCarthy III, & Jolesz, 2003 ;  Zhang, Weisser, Stilla, Prather, & Sathian, 2004 ) and olfactory (e.g.,  Bensafi et al., 2003 ;  Djordjevic, Zatorre, Petrides, Boyle, & Jones-Gotman, 2005 ) domains. 
 It is not clear how these neural representations are reconstructed. Preliminary evidence from an MEG study ( Tian & Poeppel, 2013 ) suggests that imagining speaking (articulation imagery, AI) and imagining hearing (hearing imagery, HI) differentially modulated neural responses to subsequent auditory stimuli. These distinct modulation effects by different types of imagery suggest that similar auditory neural representations may be internally formed via different neural pathways. A dual stream prediction model (DSPM,  Fig. 1 ) was proposed in which two distinct processes in parallel neural pathways can internally induce the corresponding perceptual neural representation ( Tian & Poeppel, 2012 ,  2013 ). 
 In the  simulation-estimation prediction stream  ( Fig. 1 ), the perceptual consequences of actions are predicted by simulating the movement trajectory, followed by estimating the perceptual changes that would be associated with this movement. AI has been hypothesized to implement the motor-to-sensory transformation for simulation-estimation mechanism ( Tian & Poeppel, 2013 ). Specifically, during AI, a motor simulation process similar to speech motor preparation is carried out, but without execution and output ( Palmer et al., 2001 ;  Tian & Poeppel, 2010 ,  2012 ). Therefore, neural networks that mediate motor simulation should be similar to the ones implicated in motor preparation, including supplementary motor area (SMA), inferior frontal gyrus (IFG), premotor and insula ( Bohland & Guenther, 2006 ;  Palmer et al., 2001 ;  Shuster & Lemieux, 2005 ). After motor simulation, a copy of the planned motor commands – known as the efference copy ( Von Holst & Mittelstaedt, 1950/1973 ; for a review see  Wolpert & Ghahramani, 2000 ) – is sent to the somatosensory areas and is used in a forward model to estimate the somatosensory consequences ( Blakemore & Decety, 2001 ). This somatosensory estimation is hypothesized to be governed by the networks underlying somatosensory perception ( Blakemore, Wolpert, & Frith, 1998 ;  Tian & Poeppel, 2010 ,  2012 ), including primary and secondary somatosensory regions, parietal operculum (PO) and the supramarginal gyrus (SMG). Moreover in the context of speech, we hypothesize that auditory consequences are predicted on the basis of somatosensory estimation, and this auditory estimation will recruit neural structures in temporal auditory cortices ( Tian & Poeppel, 2010 ,  2012 ,  2013 ,  2015 ). 
 In the  memory-retrieval prediction stream  ( Fig. 1 ), the internally induced neural representations are the result of memory retrieval processes – reconstructing stored perceptual information in modality-specific cortices ( Kosslyn, 1994 ,  2005 ;  Wheeler et al., 2000 ). In particular, the retrieved object properties from long-term memory reactivate the sensory cortices that originally processed the object features ( Kosslyn, 1994 ). In this experiment, we employed HI to probe this memory-retrieval stream. Auditory representations can be retrieved from various memory sources such as episodic memory, which presumably relies on hippocampal structures ( Carr, Jadhav, & Frank, 2011 ;  Eichenbaum, Sauvage, Fortin, Komorowski, & Lipton, 2012 ) with a possible buffer site in parietal cortex ( Vilberg & Rugg, 2008 ;  Wagner, Shannon, Kahn, & Buckner, 2005 ). Auditory representations can also be transformed from lexical and semantic information stored in semantic networks, including frontal (e.g., dorsomedial prefrontal cortex, IFG, ventromedial prefrontal cortex), parietal (e.g., posterior inferior parietal lobe) and temporal (e.g., middle temporal gyrus) regions ( Binder, Desai, Graves, & Conant, 2009 ;  Lau, Phillips, & Poeppel, 2008 ;  Price, 2012 ). Regardless of the divergent functional roles (episodic or semantic networks), frontal and parietal regions are reliably activated during memory retrieval processes. Therefore, neural activation in a frontal-parietal distributed network – the proposed memory-retrieval prediction stream – should be observed during HI. 
 This study uses fMRI to investigate three neuroanatomical/ functional hypotheses that are generated from the DSPM. First, if the perfect simulation-estimation and memory-retrieval tasks were carried out, two distinct processing streams would be revealed separately. However, because speech imagery could involve both production and perception, we predict that both types of imagery will activate the simulation-estimation stream for simulating speech motor action ( Tian & Poeppel, 2013 ). More importantly, we hypothesize that each type of imagery will recruit each prediction stream to a different extent. Specifically, we predict that AI will induce stronger activation in the simulation-estimation prediction stream, including SMA, IFG, premotor, insula for motor simulation, as well as primary and/or secondary somatosensory areas PO and SMG for subsequent estimation of somatosensory consequences. On the other hand, we predict that HI will have more activation in the memory-retrieval prediction stream, which is comprised of frontal, superior and inferior parietal cortices that are associated with memory retrieval ( Binder et al., 2009 ;  Lau et al., 2008 ;  Price, 2012 ;  Vilberg & Rugg, 2008 ;  Wagner et al., 2005 ). 
 Second, we suggest that a more precise, detailed auditory prediction can be induced through simulation-estimation mechanisms, comparing to that obtained via memory-retrieval route ( Hickok, 2012 ;  Oppenheim & Dell, 2010 ;  Tian & Poeppel, 2012 ,  2013 ). We propose that there is a  one-to-one mapping  between motor simulation and perceptual estimation via a bridge of somatosensory estimation in the simulation-estimation stream. Such a deterministic prediction mechanism, contrasted with the memory-retrieval prediction stream’s probabilistic prediction mechanism (narrowing down the target features in distributions of stored memory), presumably suffers less interference and lateral inhibition from similar features and yields a stronger and robust representation ( Tian & Poeppel, 2012 ,  2013 ). Based on this hypothesis of enriched auditory representations via simulation and estimation processes, we predict that auditory cortices will be more strongly activated in AI than in HI. 
 Finally, we hypothesize that the neural networks governing simulation within the simulation-estimation stream overlap with cortical regions underlying motor preparation during speech production ( Tian & Poeppel, 2012 ). That is, the initial motor processes are the same during articulation (A) and AI until the processes diverge, specifically until the motor signals are not executed during imagery. Therefore, we predict that enhanced activity in SMA, IFG, premotor areas and insula, which has been observed during preparation of overt speech production ( Brendel et al., 2010 ;  Riecker et al., 2005 ), will be observed in both AI and A. The observation of overlapping neural networks will provide evidence towards potentially shared neural mechanisms between overt and covert speech production, and furthermore suggests that mental imagery of speech is a valid paradigm to research these shared motor processes. 
 
 
 2. Methods 
 
 2.1. Participants 
 Eighteen volunteers gave informed consent and participated in the experiment (10 males, mean age 28.2 years, range 20–44 years). All participants were right-handed, with no history of neurological disorders. The experimental protocol was approved by the New York University Institutional Review Board (IRB). 
 
 
 2.2. Materials 
 Two 600-msec duration consonant-vowel syllables (/ba/,/ki/) were used as auditory stimuli (female voice; sampling rate of 48 kHz). All sounds were normalized to 70 dB SPL and delivered through MR-compatible headphones (MR confon Silenta, MR confon GmbH, Magdeburg, Germany). Four images were used as visual cues to indicate different trial types. Each image was presented foveally, against a black background, and subtended less than 10° visual angle. A label - either ‘/ba/’ or ‘/ki/’ – was superimposed on the center of each picture (<4° visual angle) to indicate the syllable that participants would produce in the following tasks. 
 
 
 2.3. Experimental procedure 
 We employed a similar experimental design as  Tian and Poeppel (2013)  (see  Fig. S1 ). The experiment was comprised of four conditions: articulation (A), hearing (H), articulation imagery (AI), and hearing imagery (HI). In A, participants were asked to overtly generate the cued syllable (gently, to minimize head movement). In AI, participants were required to imagine saying a syllable without any overt movement of the articulators. In H, participants passively listened to one of the syllables. In HI, participants were asked to imagine hearing the cued syllable. 
 The timing of trials was consistent across conditions ( Fig. S1 ). First, a visual cue appeared in the center of the screen at the beginning of each trial and stayed on for 1000 msec. During the following 2400 msec, participants actively formed a syllable in three of the task conditions (A, AI, and HI) or passively perceived an auditory syllable in H, in which a syllable was presented 1200 msec after the offset of visual cue, followed by a 600 msec interval. Notice that the 2400 msec period was the total duration that participants were allowed to finish the tasks (indicated by the curly bracket,  Fig. S1 ). The actual time of performing task was much shorter, presumably comparable to the syllable duration. Finally, participants were presented with one of the syllables that always followed the task phase. The inter-trial interval was randomly chosen from 4440 to 6660 msec (2–3 TRs, see MRI scanning for details), temporally jittered by 46.25-msec increments (length of TR divided by 48, the number of task trials in a run). Twelve trials for each of the four tasks were presented in each run. Six resting trials (length: 9550 msec), which were visually cued with the word ‘rest’ were also included in each run. In total, the experiment included five runs with 54 trials each, encompassing all four tasks and the rest condition, which were pseudo-randomly presented in each run. 
 The goal of our earlier MEG study ( Tian & Poeppel, 2013 ), on which this current study builds, was to assess cross-modal repetition adaptation. In contrast, the goal of this study is to assess the neural networks mediating internal perceptual reactivation by testing the main effects among different tasks, which are independent of the adaptation effects. Because the different syllables were used equally often, we only compared between overt and covert tasks, rather than between different syllables. 
 Each participant received training for 15–20 min before the experiment, and they focused on the timing as well as vividness of imagery. First, only the H trials were presented to introduce the relative timing among the visual cue, the first auditory stimulus (the same period for active tasks in other conditions), and the subsequent auditory stimuli. After participants were familiar with the timing, they were instructed to use similar timing for the other conditions. This was to prevent any overlap between the internally generated neural responses during tasks and the subsequent responses to the external auditory stimuli. Next, they practiced on A trials while the experimenter observed the overt articulation and provided feedback if needed; participants executed the task with similar timing and without overlaps between their voice and subsequent auditory stimuli, before they moved onto the imagery conditions. For the AI condition, they were told to imagine speaking the syllables “in their mind” without moving any articulators or producing any sounds. They should feel the movement of specific articulators that would associate with actual pronunciation. For the HI condition, they were asked to recreate the female voice from the H condition in their minds, but minimize any feeling of movement in their articulators. If needed, the recorded female voice was presented again to form a better memory. We tried to selectively elicit the motor-induced auditory representation in imagined speaking, while we aimed to target auditory memory retrieval in imagined hearing. Participants were asked to generate a movement intention and kinesthetic feeling of articulation in the AI condition; in the HI condition, such motor-related imagery activity was strongly discouraged. After verbal confirmation of successful distinction between these types of imagery formation, they further practiced on the AI and HI tasks to reinforce the vividness of imagery and the timing requirement in the trials. Lastly, they trained on a practice block in which all four conditions were presented. Timing of the A condition was monitored by the experimenter and verbal confirmation of distinction between imagery tasks was obtained for each participant before proceeding onto the main experiment. 
 
 
 2.4. fMRI data acquisition 
 Scanning was performed with a 3T Siemens Allegra MRI system using a single-channel, whole-head coil. Functional data were acquired using a gradient-echo, echo-planar pulse sequence (TR = 2220 msec; TE = 30 msec; 38 slices oriented about 30° rotated counter-clockwise from AC-PC line, which was adjusted individually to maximize coverage; (3 × 3 × 3) mm 3  voxel size, .6 mm interslice gap; 244 volume acquisitions in each of five runs). High resolution T1-weighted (MP-RAGE) images were collected from each participant for anatomical visualization. 
 
 
 2.5. fMRI pre-processing 
 Data were analyzed using SPM8 ( http://www.fil.ion.ucl.ac.uk/spm/ ). The first two volumes of a run (dummy images) were discarded from all analyses. All functional volumes were spatially realigned after motion correction. Structural images were coregistered to the functional images and spatially normalized to a T1-ICBM152 template provided in SPM8. The resulting normalization parameters were applied to the functional images, followed by spatial smoothing with an 8-mm full-width, half maximum isotropic Gaussian kernel. 
 
 
 2.6. Statistical analysis 
 Voxel-wise statistical parametric maps of brain activation were generated by estimating the parameters of a general linear model (GLM). For each of the four conditions (A, AI, H, HI) in each participant, neural activity was modeled as boxcar events spanning the entire 4 sec trial period (from onset of visual cue to the offset of auditory stimuli), convolved with a canonical hemodynamic response function, and entered as regressors into a fixed effect GLM. The time series were high-pass filtered with a cut-off at 128 sec. 
 For each comparison of interest in each participant, a contrast of parameter estimates (β weights) was calculated in a voxel-wise manner to produce a contrast image. Two groups of contrasts were defined. The first group of contrasts was the main effects of AI, HI and A: (1) A > H; (2) AI > H; (3) HI > H. Because this study was designed to assess the neural networks that mediate AI, HI, and A, H was used as a baseline to account for neural responses to the auditory stimuli that cannot be temporally separated from the responses of interest (the tasks) in the experimental design. The second group of contrasts contained direct comparisons between imagery tasks: (4) AI > HI; (5) HI > AI. These direct contrasts revealed the possible differential involvement of neural pathways in different types of speech imagery tasks. 
 The parameter estimates from these first-level analyses were then entered into a random (between-subject) effect analysis, and linear contrasts were used to identify responsive regions. Thresholded t-maps were obtained for all contrasts, with a cluster threshold of 25 contiguous voxels whose test statistic exceeded an uncorrected  p  value of .001 ( Lieberman & Cunningham, 2009 ). Because the effect size of mental imagery in each voxel is weak compared to overt hearing and production, we chose this approach to balance the type I and II errors. The AlphaSim Monte Carlo simulation in the original method paper ( Lieberman & Cunningham, 2009 ) shows that using the statistical threshold of  p  < .005 and cluster size of 10 voxels can achieve a desirable balance between type I and II errors, while using a 20 voxel extent threshold produces an actual false discovery rate (FDR) of .05. For this study, an AlphaSim Monte Carlo simulation with our particular scanning and analysis parameters – a smoothing kernel of 8 mm and voxel resolution of 3 mm – combined with a more conservative criterion with the magnitude statistical threshold of .001 and cluster threshold of 25 voxels yielded an FDR of .022. To examine regions that showed significant common neural responses to AI and HI as well as to A and AI, conjunction analyses were performed with the contrasts of interest [AI > H]∩[HI > H] and [A > H]∩[AI > H] ( Friston, Penny, & Glaser, 2005 ;  Nichols, Brett, Andersson, Wager, & Poline, 2005 ). 
 For visualization purposes, thresholded maps were superimposed on an average, spatially normalized anatomical image obtained from the 18 participants. The locations of neural activity were first classified using the Automated Anatomical Labeling (AAL) map ( Tzourio-Mazoyer et al., 2002 ), and then were further refined with: 1) neuroanatomical atlases ( Duvernoy, 1991 ;  Schmahmann et al., 1999 ); 2) probabilistic maps or profiles for primary auditory cortex ( Penhune, Zatorre, MacDonald, & Evans, 1996 ), planum temporale ( Westbury, Zatorre, & Evans, 1999 ), pars opercularis of IFG ( Tomaiuolo et al., 1999 ), and mouth region of primary motor cortex ( Fox et al., 2001 ); and 3) locations defined by previous reports or reviews on the medial frontal and cingulate areas ( Picard & Strick, 1996 ,  2001 ) and subdivisions of the premotor cortex ( Chen, Penhune, & Zatorre, 2008 ). 
 
 
 
 3. Results 
 
 3.1. Main effects of tasks A, AI and HI 
 Speech production networks were observed during A, which included bilateral anterior cingulate cortex (ACC), pre-SMA/ SMA complex, sensorimotor cortex, middle frontal cortex (BA 46) and right posterior cingulate cortex. Cerebellum and subcortical regions, including thalamus and basal ganglia were also activated (see  Table S1  for a complete activation list). Significant activations, as well as in all following analyses, surpassed a threshold of  t  > 3.65 ( p  < .001 uncorrected) with an extent of at least 25 voxels which is equivalent to FDR smaller than .05. 
 The neural networks that mediated AI were comprised of frontal and motor-related regions, including bilateral pre-SMA, inferior frontal pars opercularis (BA 44), frontal operculum, anterior insula, mid premotor cortex (BA 6), middle frontal cortex (BA 46); this activity extended to left primary motor cortex (near the mouth region). Parietal activation was observed in the left parietal operculum. Moreover, activity in bilateral cerebellum VI (declive) and globus pallidus was also observed (see  Table S2  for a complete list of activation). Activity in auditory cortices was not observed in the contrast of AI-H ( Fig. S2 ). 
 Similar neural networks were observed during the HI task, including bilateral pre-SMA/SMA, inferior frontal pars opercularis (BA 44), frontal operculum, mid premotor cortex (BA 6) in the frontal lobe, and left parietal operculum in the parietal lobe. Bilateral cerebellum VI (declive) was also engaged (see  Table S3  for a complete list of activation). Activity in auditory cortices was not observed in the contrast of HI-H ( Fig. S2 ). 
 
 
 3.2. Shared regions for AI and HI 
 The conjunction analysis between AI and HI revealed the shared neural networks for these imagery tasks ( Fig. 2a ): bilateral pre-SMA/SMA, inferior frontal pars opercularis (BA 44), and left anterior insula, mid premotor cortex (BA 6) extending to primary motor cortex (near mouth region), and bilateral cerebellum VI (declive). Moreover, activity from both tasks overlapped in left parietal operculum, a somatosensory-related area (see  Table 1  for a complete list of activation peaks). 
 
 
 3.3. Stronger neural activity during HI 
 In the direct contrast between HI and AI, stronger activity was observed in HI compared with AI in left middle frontal (BA 8) and in left inferior parietal cortex and intraparietal sulcus ( Fig. 2b , also see  Table 2  for a complete list). Activity in auditory cortices was not observed in the contrast of HI-AI ( Fig. S2 ). 
 
 
 3.4. Stronger activation during AI 
 The direct comparison between AI and HI revealed stronger activation during AI over frontal and parietal areas, including bilateral sensorimotor cortex, left subcentral gyrus (Rolandic operculum, encompassing vocalization areas of primary motor and somatosensory cortex) and middle frontal cortex (BA 46), as well as left parietal operculum ( Fig. 2c , also see  Table 3 ). The same direct comparison between AI and HI also revealed stronger activation during AI over temporal cortices, including left anterior superior temporal gyrus (aSTG) and right posterior superior temporal sulcus (pSTS) ( Fig. 2c , also see  Table 3 ). 
 
 
 3.5. Common neural networks that mediate A and AI 
 The conjunction analysis between A and AI revealed the shared networks between overt and covert speech production tasks ( Fig. 3 ). These overlapping areas included bilateral ACC, inferior frontal pars opercularis (BA 44), pre-SMA/SMA, left mid-dorsal insula, right frontal operculum and left parietal operculum, as well as bilateral cerebellum VI (declive), globus pallidus and left putamen ( Table 4 ). 
 
 
 
 4. Discussion 
 We investigated the neural networks that mediate perceptual reactivation using fMRI with speech imagery paradigms. Whereas the neural networks that mediate AI and HI largely overlapped in frontal-parietal motor-sensory areas, different subsets of frontal and parietal regions were involved in each task. This differential involvement of neural networks suggests two possible mechanisms for reactivating perceptual neural representation. 
 Frontal-parietal neural networks were observed during both AI and HI. The frontal overlapped areas included bilateral pre-SMA/SMA, inferior frontal pars opercularis (BA 44), and left anterior insula, mid premotor cortex (BA 6) (see  Fig. 2a ). Interestingly, most of the observed overlapped networks between AI and HI (BA 44, pre-SMA/SMA, insula) were also found in the conjunction analysis between AI and actual articulation (A) (see  Fig. 3 ). These frontal/insular regions (SMA, IFG, premotor and insula) have been implicated in motor preparation during overt speech production ( Bohland & Guenther, 2006 ;  Palmer et al., 2001 ;  Shuster & Lemieux, 2005 ). Therefore, perceptual reactivation processes during AI and HI may recruit these regions to simulate motor preparation without motor execution ( Palmer et al., 2001 ;  Tian & Poeppel, 2010 ,  2012 ); motor simulation may then induce activity in sensory cortices. In fact, aside from the observed shared frontal activity between AI and HI, we also observed overlap activation in PO (see  Fig. 2a ), an area that relates to somatosensory perception (e.g.,  Blakemore et al., 1998 ). These results are consistent with the internal forward model theory, which hypothesizes that a copy of the planned motor commands – known as the efference copy ( Von Holst & Mittelstaedt, 1950/1973 ; for a review see  Wolpert & Ghahramani, 2000 ) – is sent to somatosensory areas and is used to estimate the somatosensory consequences of an action ( Blakemore & Decety, 2001 ). Therefore, the observed activation in frontal-parietal sensorimotor regions during both AI and HI suggests that the motor-sensory interaction via the simulation-estimation process is a potential top-down mechanism to reactivate sensory cortices without external stimuli or output. 
 Multiple functions such as auditory working memory have been associated with the SMG (e.g.,  Paulesu, Frith, & Frackowiak, 1993 ). In this study, the PO, an area close to SMG was observed in both articulation and AI conditions. In the articulation condition, participants were required to say only one syllable after visual cue, so the working memory demand was minimal. As such, the observed parietal opercular activity may not have been elicited by working memory, but rather perception of somatosensory feedback. Together with the conjunction results, the observed parietal opercular activation in AI may be involved in the estimation of somatosensory consequences in a process similar to that seen during somatosensory perception. 
 The direct contrast between AI and HI reveals that frontal-parietal sensorimotor regions were activated stronger during AI (see  Fig. 2c ). The observed greater activation in frontal and motor areas during AI, including bilateral sensorimotor cortex, left subcentral gyrus (Rolandic operculum) and middle frontal cortex (BA 46) is similar to activation patterns indicative of articulation preparation ( Brendel et al., 2010 ;  Price, 2012 ;  Riecker et al., 2005 ), which suggests that AI relies more on internally simulating articulatory preparation. Additionally, greater parietal opercular activity during AI may represent stronger somatosensory estimation during motor simulation. 
 On the other hand, the reverse comparison between HI and AI revealed that activity increased in left middle frontal, inferior parietal cortex and intraparietal sulcus during HI, which may form a subset of proposed distributed memory systems. For example, parietal cortex has been hypothesized as a buffer site for episodic memory ( Vilberg & Rugg, 2008 ;  Wagner et al., 2005 ). Lexical and semantic information may be stored in semantic networks, including frontal (e.g., dorsomedial prefrontal cortex, IFG, ventromedial prefrontal cortex) and parietal (e.g., posterior inferior parietal lobe) regions ( Binder et al., 2009 ;  Price, 2012 ). The greater activity seen in middle frontal, inferior parietal cortex and intraparietal sulcus during HI may reflect memory retrieval during perceptual reactivation. That is, HI may also rely on two complementary processes: a memory retrieval operation and motor simulation. This combined contribution from motor and memory systems in reactivated perceptual representation may be due to the nature of the HI task: both speech perception and production are related to this particular process of perceptual reactivation, and hence need memory retrieval of information related to speech perception and motor processes to simulate speech production. Therefore, this dissociation of neural pathways between AI and HI tasks implies that (1) two functional pathways exist for perceptual reactivation: one underlies motor-to-perceptual transformation and another mediates memory retrieval; and (2) these two pathways are differentially recruited during perceptual reactivation for different imagery tasks. 
 Stronger activity in bilateral temporal auditory regions and frontal-parietal sensorimotor systems was observed during AI, compared to activity recruited for both sensorimotor activation and memory retrieval during HI. This supports the hypothesis that detailed auditory representations can be reactivated by the one-to-one ‘deterministic’ mapping between motor and perceptual systems ( Tian & Poeppel, 2012 ,  2013 ). This mapping structure provides motor-to-sensory transformation dynamics that enrich the details of the representation, leading perhaps from phonemic to phonetic levels of detail ( Hickok, 2012 ), which may not be available during memory retrieval. This result is also consistent with the behavioral observation that motor engagement enriches phonetic details, which can then influence speech-error rates at lexical-phonological and phonemic-articulatory levels during a covert tongue twister task ( Oppenheim & Dell, 2010 ). 
 STS recruitment is commonly observed in speech and song production studies that manipulate auditory feedback (e.g.,  Niziolek & Guenther, 2013 ;  Tourville, Reilly, & Guenther, 2008 ;  Zarate, Wood, & Zatorre, 2010 ;  Zarate & Zatorre, 2008 ;  Zheng, Munhall, & Johnsrude, 2010 ). The observation of increased STS activity during AI in our study suggests similar computations between AI and self-monitoring during speech production. Whereas the auditory feedback manipulation during speech production actually creates the discrepancy between expectation and auditory input, the lack of auditory feedback during AI can also be considered as a similar violation of a sensory expectation generated after motor preparation and simulation. To support this, the location of our STS activation in the AI task [54, −26, 2] resembles the locations of STS activity reported in feedback perturbation studies: [52.8, −32.1, 4.4] in ( Niziolek & Guenther, 2013 ), [58, −28, 6] in ( Tourville et al., 2008 ), and [54, −18, −6] in ( Zheng et al., 2010 ). Therefore, we suggest that similar mechanisms for generating auditory predictions and subsequent comparisons with incoming auditory feedback may be carried out in STS during both speech imagery and speech monitoring. 
 Price (2012)  implicates the aSTG in the early auditory processing of complex sounds. This anterior region of temporal gyrus has been found to be sensitive to rapid frequency transition ( Belin et al., 1998 ) and spectral variation ( Zatorre & Belin, 2001 ). Rapid frequency modulations are a key feature in words that might need to be internally reconstructed and parsed to distinguish between particular phonemes or syllables in speech. Thus, the observation of increased aSTG activity during AI might suggest that auditory representations of spectral transitions, similar to those seen during speech perception, can be internally induced without any external stimulation. 
 Our observed increase in activity within associative auditory cortices aSTG and pSTS (but not within primary auditory cortex) during speech imagery is consistent with findings in earlier auditory imagery studies (e.g.,  Bunzeck, Wuestenberg, Lutz, Heinze, & Jancke, 2005 ;  Halpern & Zatorre, 1999 ;  Herholz, Halpern, & Zatorre, 2012 ;  Shergill et al., 2001 ;  Zatorre et al., 1996 ). It should be noted, however, that some auditory imagery work has reported primary auditory cortex activation (e.g.,  Kraemer et al., 2005 ; see  Zatorre & Halpern, 2005  for a review), but we speculate that imagining different levels of content complexity may require multiple levels of auditory processing, which could result in the recruitment of different stages along the auditory perceptual hierarchy. In the current study, we use spoken syllables as stimuli. Given the complex nature of these stimuli, the internal reconstruction of syllabic representation may occur beyond the computations and representations that are mediated by primary auditory cortex. Our MEG studies support this view – the response latencies were modulated by the content of stimuli, for example at 200 ms for syllables ( Tian & Poeppel, 2013 ) and 100 msec for pitch ( Tian & Poeppel, 2015 ), suggesting that simpler stimuli may only recruit lower (and therefore faster) level areas for auditory processing, whereas more complex stimuli recruit higher-order auditory regions within the auditory perceptual hierarchy and thus require additional processing time. Additional studies will need to be conducted to determine whether an auditory processing hierarchy can be accessed by increasingly complex imagined stimuli, as has been reported in the visual domain ( Kosslyn & Thompson, 2003 ). 
 In summary, this study complements and extends beyond our earlier MEG study ( Tian & Poeppel, 2013 ) by offering neuroanatomical evidence that supports the existence of two complementary neural pathways for perceptual reactivation. Two speech imagery tasks differentially recruit a motor-to-sensory transformation pathway and a memory-retrieval pathway. Moreover, stronger auditory responses in AI suggest that motor system involvement leads to stronger perceptual reactivation. 
 
 
 Supplementary Material 
 
 Supplemental Materials 
 
 
 
 
 
 We thank Keith Sanzenbach for his technical assistance with fMRI recording, Tobias Overath and Thomas Schofield for their discussion and guidance with fMRI analyses, and Jess Rowland for her comments on this manuscript. This study was supported by MURI ARO #54228-LS-MUR, NIH 2R01DC 05660, a grant from the GRAMMY Foundation ® , Major Projects Program of the Shanghai Municipal Science and Technology Commission (STCSM) 15JC1400104 and National Natural Science Foundation of China 31500914. 
 
 
 
 
 Supplementary data 
 
 Supplementary data  related to this article can be found at  http://dx.doi.org/10.1016/j.cortex.2016.01.002 . 
 
 
 
 
 
 
 
 Albright 
 TD 
 
 
 2012 
 On the perception of probable things: neural substrates of associative memory, imagery, and perception 
 Neuron 
 74 
 2 
 227 
 245 
 22542178 
 
 
 
 
 Aristotle 
 1986 
 De Anima (on the soul) 
 
 
 Lawson-Tancred 
 H 
 
 
 London 
 Penguin Books 
 
 
 
 
 
 
 Belin 
 P 
 
 
 Zilbovicius 
 M 
 
 
 Crozier 
 S 
 
 
 Thivard 
 L 
 
 
 Fontaine 
 A 
 
 
 Masure 
 M-C 
 
 
 
 1998 
 Lateralization of speech and auditory temporal processing 
 Journal of Cognitive Neuroscience 
 10 
 4 
 536 
 540 
 9712682 
 
 
 
 
 
 
 Bensafi 
 M 
 
 
 Porter 
 J 
 
 
 Pouliot 
 S 
 
 
 Mainland 
 J 
 
 
 Johnson 
 B 
 
 
 Zelano 
 C 
 
 
 
 2003 
 Olfactomotor activity during imagery mimics that during perception 
 Nature neuroscience 
 6 
 11 
 1142 
 1144 
 14566343 
 
 
 
 
 
 
 Berkeley 
 G 
 
 
 1734/1965a 
 Three dialogues between Hylas and Philonus 
 
 
 Turbayne 
 CM 
 
 
 Principles, dialogues, and correspondence 
 Indianapolis 
 Bobbs-Merrill 
 
 
 
 
 
 
 Berkeley 
 G 
 
 
 1734/1965b 
 A treatise concerning the principles of human knowledge 
 
 
 Turbayne 
 CM 
 
 
 Principles, dialogues, and correspondence 
 Indianapolis 
 Bobbs-Merrill 
 
 
 
 
 
 
 Binder 
 JR 
 
 
 Desai 
 RH 
 
 
 Graves 
 WW 
 
 
 Conant 
 LL 
 
 
 2009 
 Where is the semantic system? A critical review and meta-analysis of 120 functional neuroimaging studies 
 Cerebral Cortex 
 19 
 12 
 2767 
 2796 
 19329570 
 
 
 
 
 
 
 Blakemore 
 SJ 
 
 
 Decety 
 J 
 
 
 2001 
 From the perception of action to the understanding of intention 
 Nature Reviews Neuroscience 
 2 
 8 
 561 
 567 
 11483999 
 
 
 
 
 
 
 Blakemore 
 SJ 
 
 
 Wolpert 
 DM 
 
 
 Frith 
 CD 
 
 
 1998 
 Central cancellation of self-produced tickle sensation 
 Nature Neuroscience 
 1 
 7 
 635 
 640 
 10196573 
 
 
 
 
 
 
 Bohland 
 JW 
 
 
 Guenther 
 FH 
 
 
 2006 
 An fMRI investigation of syllable sequence production 
 NeuroImage 
 32 
 2 
 821 
 841 
 16730195 
 
 
 
 
 
 
 Brendel 
 B 
 
 
 Hertrich 
 I 
 
 
 Erb 
 M 
 
 
 Lindner 
 A 
 
 
 Riecker 
 A 
 
 
 Grodd 
 W 
 
 
 
 2010 
 The contribution of mesiofrontal cortex to the preparation and execution of repetitive syllable productions: an fMRI study 
 NeuroImage 
 50 
 3 
 1219 
 1230 
 20080191 
 
 
 
 
 
 
 Bunzeck 
 N 
 
 
 Wuestenberg 
 T 
 
 
 Lutz 
 K 
 
 
 Heinze 
 HJ 
 
 
 Jancke 
 L 
 
 
 2005 
 Scanning silence: mental imagery of complex sounds 
 NeuroImage 
 26 
 4 
 1119 
 1127 
 15893474 
 
 
 
 
 
 
 Carr 
 MF 
 
 
 Jadhav 
 SP 
 
 
 Frank 
 LM 
 
 
 2011 
 Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval 
 Nature Neuroscience 
 14 
 2 
 147 
 153 
 21270783 
 
 
 
 
 
 
 Chen 
 JL 
 
 
 Penhune 
 VB 
 
 
 Zatorre 
 RJ 
 
 
 2008 
 Listening to musical rhythms recruits motor regions of the brain 
 Cerebral Cortex 
 18 
 12 
 2844 
 2854 
 18388350 
 
 
 
 
 
 
 Descartes 
 R 
 
 
 1642/1984 
 Meditations on first philosophy 
 
 
 Cottingham 
 J 
 
 
 Stoothoff 
 R 
 
 
 Murdoch 
 D 
 
 
 The philosophical writings of Descartes 
 2 
 Cambridge 
 Cambridge University Press 
 
 
 
 
 
 
 Djordjevic 
 J 
 
 
 Zatorre 
 RJ 
 
 
 Petrides 
 M 
 
 
 Boyle 
 J 
 
 
 Jones-Gotman 
 M 
 
 
 2005 
 Functional neuroimaging of odor imagery 
 NeuroImage 
 24 
 3 
 791 
 801 
 15652314 
 
 
 
 
 
 
 Duvernoy 
 H 
 
 
 1991 
 The human brain: Structure, three-dimensional sectional anatomy and MRI 
 Wien 
 Springer-Verlag 
 
 
 
 
 
 
 Eichenbaum 
 H 
 
 
 Sauvage 
 M 
 
 
 Fortin 
 N 
 
 
 Komorowski 
 R 
 
 
 Lipton 
 P 
 
 
 2012 
 Towards a functional organization of episodic memory in the medial temporal lobe 
 Neuroscience & Biobehavioral Reviews 
 36 
 7 
 1597 
 1608 
 21810443 
 
 
 
 
 
 
 Fox 
 PT 
 
 
 Huang 
 A 
 
 
 Parsons 
 LM 
 
 
 Xiong 
 JH 
 
 
 Zamarippa 
 F 
 
 
 Rainey 
 L 
 
 
 
 2001 
 Location-probability profiles for the mouth region of human primary motor–sensory cortex: model and validation 
 Neuroimage 
 13 
 1 
 196 
 209 
 11133322 
 
 
 
 
 
 
 Friston 
 KJ 
 
 
 Penny 
 WD 
 
 
 Glaser 
 DE 
 
 
 2005 
 Conjunction revisited 
 NeuroImage 
 25 
 3 
 661 
 667 
 15808967 
 
 
 
 
 
 
 Halpern 
 AR 
 
 
 Zatorre 
 RJ 
 
 
 1999 
 When that tune runs through your head: a PET investigation of auditory imagery for familiar melodies 
 Cerebral Cortex 
 9 
 7 
 697 
 704 
 10554992 
 
 
 
 
 
 
 Herholz 
 SC 
 
 
 Halpern 
 AR 
 
 
 Zatorre 
 RJ 
 
 
 2012 
 Neuronal correlates of perception, imagery, and memory for familiar tunes 
 Journal of Cognitive Neuroscience 
 24 
 6 
 1382 
 1397 
 22360595 
 
 
 
 
 
 
 Hickok 
 G 
 
 
 2012 
 Computational neuroanatomy of speech production 
 Nature Reviews Neuroscience 
 13 
 2 
 135 
 145 
 22218206 
 
 
 
 
 
 
 Hobbes 
 T 
 
 
 1651/1968 
 
 
 Macpherson 
 C 
 
 
 Leviathan 
 London 
 Penguin Books 
 
 
 
 
 
 
 Hume 
 D 
 
 
 1969 
 
 
 Mossner 
 EC 
 
 
 A treatise of human nature 
 London 
 Penguin Books 
 
 
 
 
 
 
 James 
 W 
 
 
 1890 
 The Principles of Psychology 
 2 
 London 
 MacMillan 
 
 
 
 
 
 
 Ji 
 D 
 
 
 Wilson 
 MA 
 
 
 2006 
 Coordinated memory replay in the visual cortex and hippocampus during sleep 
 Nature Neuroscience 
 10 
 1 
 100 
 107 
 17173043 
 
 
 
 
 
 
 Kosslyn 
 SM 
 
 
 1994 
 Image and brain: The resolution of the imagery debate 
 Cambridge, MA 
 MIT Press 
 
 
 
 
 
 
 Kosslyn 
 SM 
 
 
 2005 
 Mental images and the brain 
 Cognitive Neuropsychology 
 22 
 3–4 
 333 
 347 
 21038254 
 
 
 
 
 
 
 Kosslyn 
 SM 
 
 
 Ganis 
 G 
 
 
 Thompson 
 WL 
 
 
 2001 
 Neural foundations of imagery 
 Nature Reviews Neuroscience 
 2 
 9 
 635 
 642 
 11533731 
 
 
 
 
 
 
 Kosslyn 
 SM 
 
 
 Pascual-Leone 
 A 
 
 
 Felician 
 O 
 
 
 Camposano 
 S 
 
 
 Keenan 
 J 
 
 
 Ganis 
 G 
 
 
 
 1999 
 The role of area 17 in visual imagery: convergent evidence from PET and rTMS 
 Science 
 284 
 5411 
 167 
 10102821 
 
 
 
 
 
 
 Kosslyn 
 SM 
 
 
 Thompson 
 WL 
 
 
 2003 
 When is early visual cortex activated during visual mental imagery? 
 Psychological Bulletin 
 129 
 5 
 723 
 12956541 
 
 
 
 
 
 
 Kraemer 
 DJM 
 
 
 Macrae 
 CN 
 
 
 Green 
 AE 
 
 
 Kelley 
 WM 
 
 
 2005 
 Musical imagery: sound of silence activates auditory cortex 
 Nature 
 434 
 7030 
 158 
 158 
 15758989 
 
 
 
 
 
 
 Lau 
 EF 
 
 
 Phillips 
 C 
 
 
 Poeppel 
 D 
 
 
 2008 
 A cortical network for semantics:(de) constructing the N400 
 Nature Reviews Neuroscience 
 9 
 12 
 920 
 933 
 19020511 
 
 
 
 
 
 
 Lieberman 
 MD 
 
 
 Cunningham 
 WA 
 
 
 2009 
 Type I and Type II error concerns in fMRI research: re-balancing the scale 
 Social Cognitive and Affective Neuroscience 
 4 
 4 
 423 
 428 
 20035017 
 
 
 
 
 
 
 Moulton 
 ST 
 
 
 Kosslyn 
 SM 
 
 
 2009 
 Imagining predictions: mental imagery as mental emulation 
 Philosophical Transactions of the Royal Society B: Biological Sciences 
 364 
 1521 
 1273 
 1280 
 
 
 
 
 
 
 Nichols 
 T 
 
 
 Brett 
 M 
 
 
 Andersson 
 J 
 
 
 Wager 
 T 
 
 
 Poline 
 JB 
 
 
 2005 
 Valid conjunction inference with the minimum statistic 
 NeuroImage 
 25 
 3 
 653 
 660 
 15808966 
 
 
 
 
 
 
 Niziolek 
 CA 
 
 
 Guenther 
 FH 
 
 
 2013 
 Vowel category boundaries enhance cortical and behavioral responses to speech feedback alterations 
 The Journal of Neuroscience 
 33 
 29 
 12090 
 12098 
 23864694 
 
 
 
 
 
 
 O’Craven 
 KM 
 
 
 Kanwisher 
 N 
 
 
 2000 
 Mental imagery of faces and places activates corresponding stimulus-specific brain regions 
 Journal of Cognitive Neuroscience 
 12 
 6 
 1013 
 1023 
 11177421 
 
 
 
 
 
 
 Oppenheim 
 GM 
 
 
 Dell 
 GS 
 
 
 2010 
 Motor movement matters: the flexible abstractness of inner speech 
 Memory & cognition 
 38 
 8 
 1147 
 1160 
 21156877 
 
 
 
 
 
 
 Paivio 
 A 
 
 
 1971 
 Imagery and verbal processes 
 New York 
 Holt, Rinehart & Winston 
 
 
 
 
 
 
 Paivio 
 A 
 
 
 1986 
 Mental representations: a dual coding approach 
 New York 
 Oxford University Press 
 
 
 
 
 
 
 Palmer 
 ED 
 
 
 Rosen 
 HJ 
 
 
 Ojemann 
 JG 
 
 
 Buckner 
 RL 
 
 
 Kelley 
 WM 
 
 
 Petersen 
 SE 
 
 
 2001 
 An event-related fMRI study of overt and covert word stem completion 
 NeuroImage 
 14 
 1 
 182 
 193 
 11525327 
 
 
 
 
 
 
 Paulesu 
 E 
 
 
 Frith 
 CD 
 
 
 Frackowiak 
 RS 
 
 
 1993 
 The neural correlates of the verbal component of working memory 
 Nature 
 362 
 6418 
 342 
 345 
 8455719 
 
 
 
 
 
 
 Penhune 
 V 
 
 
 Zatorre 
 RJ 
 
 
 MacDonald 
 J 
 
 
 Evans 
 A 
 
 
 1996 
 Interhemispheric anatomical differences in human primary auditory cortex: probabilistic mapping and volume measurement from magnetic resonance scans 
 Cerebral Cortex 
 6 
 5 
 661 
 672 
 8921202 
 
 
 
 
 
 
 Picard 
 N 
 
 
 Strick 
 PL 
 
 
 1996 
 Motor areas of the medial wall: a review of their location and functional activation 
 Cerebral Cortex 
 6 
 3 
 342 
 353 
 8670662 
 
 
 
 
 
 
 Picard 
 N 
 
 
 Strick 
 PL 
 
 
 2001 
 Imaging the premotor areas 
 Current Opinion in Neurobiology 
 11 
 6 
 663 
 672 
 11741015 
 
 
 
 
 Plato 
 1987 
 Theaetetus 
 
 
 Waterfield 
 RA 
 
 
 London 
 Penguin Books 
 
 
 
 
 
 
 Price 
 CJ 
 
 
 2012 
 A review and synthesis of the first 20years of PET and fMRI studies of heard speech, spoken language and reading 
 NeuroImage 
 62 
 2 
 816 
 847 
 22584224 
 
 
 
 
 
 
 Pylyshyn 
 Z 
 
 
 1981 
 The imagery debate: analogue media versus tacit knowledge 
 Psychological Review 
 88 
 1 
 16 
 45 
 
 
 
 
 
 
 Pylyshyn 
 Z 
 
 
 2003 
 Return of the mental image: are there really pictures in the brain? 
 Trends in cognitive sciences 
 7 
 3 
 113 
 118 
 12639692 
 
 
 
 
 
 
 Riecker 
 A 
 
 
 Mathiak 
 K 
 
 
 Wildgruber 
 D 
 
 
 Erb 
 M 
 
 
 Hertrich 
 I 
 
 
 Grodd 
 W 
 
 
 
 2005 
 fMRI reveals two distinct cerebral networks subserving speech motor control 
 Neurology 
 64 
 4 
 700 
 706 
 15728295 
 
 
 
 
 
 
 Schacter 
 DL 
 
 
 Addis 
 DR 
 
 
 Hassabis 
 D 
 
 
 Martin 
 VC 
 
 
 Spreng 
 RN 
 
 
 Szpunar 
 KK 
 
 
 2012 
 The future of memory: remembering, imagining, and the brain 
 Neuron 
 76 
 4 
 677 
 694 
 23177955 
 
 
 
 
 
 
 Schmahmann 
 JD 
 
 
 Doyon 
 J 
 
 
 McDonald 
 D 
 
 
 Holmes 
 C 
 
 
 Lavoie 
 K 
 
 
 Hurwitz 
 AS 
 
 
 
 1999 
 Three-dimensional MRI atlas of the human cerebellum in proportional stereotaxic space 
 NeuroImage 
 10 
 3 
 233 
 260 
 10458940 
 
 
 
 
 
 
 Shergill 
 SS 
 
 
 Bullmore 
 E 
 
 
 Brammer 
 M 
 
 
 Williams 
 S 
 
 
 Murray 
 R 
 
 
 McGuire 
 P 
 
 
 2001 
 A functional study of auditory verbal imagery 
 Psychological Medicine 
 31 
 02 
 241 
 253 
 11232912 
 
 
 
 
 
 
 Shuster 
 LI 
 
 
 Lemieux 
 SK 
 
 
 2005 
 An fMRI investigation of covertly and overtly produced mono-and multisyllabic words 
 Brain and Language 
 93 
 1 
 20 
 31 
 15766765 
 
 
 
 
 
 
 Tian 
 X 
 
 
 Poeppel 
 D 
 
 
 2010 
 Mental imagery of speech and movement implicates the dynamics of internal forward models 
 Frontiers in Psychology 
 1 
 166 
 
 http://dx.doi.org/10.3389/fpsyg.2010.00166 
 
 
 
 
 
 
 
 Tian 
 X 
 
 
 Poeppel 
 D 
 
 
 2012 
 Mental imagery of speech: linking motor and sensory systems through internal simulation and estimation 
 Frontiers in Human Neuroscience 
 6 
 314 
 
 http://dx.doi.org/10.3389/fnhum.2012.00314 
 
 
 
 
 
 
 
 Tian 
 X 
 
 
 Poeppel 
 D 
 
 
 2013 
 The effect of imagination on stimulation: the functional specificity of efference copies in speech processing 
 Journal of Cognitive Neuroscience 
 25 
 7 
 1020 
 1036 
 23469885 
 
 
 
 
 
 
 Tian 
 X 
 
 
 Poeppel 
 D 
 
 
 2015 
 Dynamics of self-monitoring and error detection in speech production: evidence from mental imagery and MEG 
 Journal of Cognitive Neuroscience 
 27 
 2 
 352 
 364 
 25061925 
 
 
 
 
 
 
 Tomaiuolo 
 F 
 
 
 MacDonald 
 J 
 
 
 Caramanos 
 Z 
 
 
 Posner 
 G 
 
 
 Chiavaras 
 M 
 
 
 Evans 
 AC 
 
 
 
 1999 
 Morphology, morphometry and probability mapping of the pars opercularis of the inferior frontal gyrus: an in vivo MRI analysis 
 European Journal of Neuroscience 
 11 
 9 
 3033 
 3046 
 10510168 
 
 
 
 
 
 
 Tourville 
 JA 
 
 
 Reilly 
 KJ 
 
 
 Guenther 
 FH 
 
 
 2008 
 Neural mechanisms underlying auditory feedback control of speech 
 NeuroImage 
 39 
 3 
 1429 
 1443 
 18035557 
 
 
 
 
 
 
 Tzourio-Mazoyer 
 N 
 
 
 Landeau 
 B 
 
 
 Papathanassiou 
 D 
 
 
 Crivello 
 F 
 
 
 Etard 
 O 
 
 
 Delcroix 
 N 
 
 
 Joliot 
 M 
 
 
 2002 
 Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain 
 NeuroImage 
 15 
 1 
 273 
 289 
 11771995 
 
 
 
 
 
 
 Vilberg 
 KL 
 
 
 Rugg 
 MD 
 
 
 2008 
 Memory retrieval and the parietal cortex: a review of evidence from a dual-process perspective 
 Neuropsychologia 
 46 
 7 
 1787 
 1799 
 18343462 
 
 
 
 
 
 
 Von Holst 
 E 
 
 
 Mittelstaedt 
 H 
 
 
 1950/1973 
 The reafference principle 
 
 
 Martin 
 R 
 
 
 The behavioral physiology of animals and man: The collected papers of Erich von Holst 
 139 
 173 
 University of Miami Press 
 
 
 
 
 
 
 Wagner 
 AD 
 
 
 Shannon 
 BJ 
 
 
 Kahn 
 I 
 
 
 Buckner 
 RL 
 
 
 2005 
 Parietal lobe contributions to episodic memory retrieval 
 Trends in Cognitive Sciences 
 9 
 9 
 445 
 453 
 16054861 
 
 
 
 
 
 
 Westbury 
 C 
 
 
 Zatorre 
 RJ 
 
 
 Evans 
 A 
 
 
 1999 
 Quantifying variability in the planum temporale: a probability map 
 Cerebral Cortex 
 9 
 4 
 392 
 405 
 10426418 
 
 
 
 
 
 
 Wheeler 
 ME 
 
 
 Petersen 
 SE 
 
 
 Buckner 
 RL 
 
 
 2000 
 Memory’s echo: vivid remembering reactivates sensory-specific cortex 
 Proceedings of the National Academy of Sciences 
 97 
 20 
 11125 
 11129 
 
 
 
 
 
 
 Wolpert 
 DM 
 
 
 Ghahramani 
 Z 
 
 
 2000 
 Computational principles of movement neuroscience 
 Nature Neuroscience 
 3 
 1212 
 1217 
 11127840 
 
 
 
 
 
 
 Wundt 
 WM 
 
 
 1913 
 Elemente der Völkerpsychologie: Grundlinien einer psychologischen Entwicklungsgeschichte der Menschheit 
 Leipzig 
 Kröner 
 
 
 
 
 
 
 Yoo 
 SS 
 
 
 Freeman 
 DK 
 
 
 McCarthy 
 JJ 
 III 
 
 
 Jolesz 
 FA 
 
 
 2003 
 Neural substrates of tactile imagery: a functional MRI study 
 NeuroReport 
 14 
 4 
 581 
 12657890 
 
 
 
 
 
 
 Zarate 
 JM 
 
 
 Wood 
 S 
 
 
 Zatorre 
 RJ 
 
 
 2010 
 Neural networks involved in voluntary and involuntary vocal pitch regulation in experienced singers 
 Neuropsychologia 
 48 
 2 
 607 
 618 
 19896958 
 
 
 
 
 
 
 Zarate 
 JM 
 
 
 Zatorre 
 RJ 
 
 
 2008 
 Experience-dependent neural substrates involved in vocal pitch regulation during singing 
 NeuroImage 
 40 
 4 
 1871 
 1887 
 18343163 
 
 
 
 
 
 
 Zatorre 
 RJ 
 
 
 Belin 
 P 
 
 
 2001 
 Spectral and temporal processing in human auditory cortex 
 Cerebral Cortex 
 11 
 10 
 946 
 953 
 11549617 
 
 
 
 
 
 
 Zatorre 
 RJ 
 
 
 Halpern 
 AR 
 
 
 2005 
 Mental concerts: musical imagery and auditory cortex 
 Neuron 
 47 
 1 
 9 
 12 
 15996544 
 
 
 
 
 
 
 Zatorre 
 RJ 
 
 
 Halpern 
 AR 
 
 
 Perry 
 DW 
 
 
 Meyer 
 E 
 
 
 Evans 
 AC 
 
 
 1996 
 Hearing in the mind’s ear: a PET investigation of musical imagery and perception 
 Journal of Cognitive Neuroscience 
 8 
 1 
 29 
 46 
 23972234 
 
 
 
 
 
 
 Zhang 
 M 
 
 
 Weisser 
 VD 
 
 
 Stilla 
 R 
 
 
 Prather 
 S 
 
 
 Sathian 
 K 
 
 
 2004 
 Multisensory cortical processing of object shape and its relation to mental imagery 
 Cognitive, Affective, & Behavioral Neuroscience 
 4 
 2 
 251 
 259 
 
 
 
 
 
 
 Zheng 
 ZZ 
 
 
 Munhall 
 KG 
 
 
 Johnsrude 
 IS 
 
 
 2010 
 Functional overlap between regions involved in speech perception and in monitoring one’s own voice during speech production 
 Journal of Cognitive Neuroscience 
 22 
 8 
 1770 
 1781 
 19642886 
 
 
 
 
 
 
 Fig. 1 
 
 Dual stream prediction model (DSPM). Top: approximate cortical regions in the hypothesized dual streams. Bottom: schematic diagram of the DSPM (color scheme corresponds to the anatomical locations above). The abstract auditory representations (orange) can be induced via perception and perceptual reactivation and are formed around STG and STS. The perceptual reactivation process can be carried out in either the memory-retrieval or simulation-estimation prediction pathway. The memory-retrieval stream (blue) includes pMTG, MTL and distributed frontal-parietal networks for retrieval from long-term lexical items, episodic and semantic memory, respectively. The simulation-estimation stream (red) includes the frontal motor system and parietal somatosensory system. The articulatory trajectory is planned in frontal motor regions, including IFG, PMC, INS and SMA. If covert production is the goal, the planned articulation signal bypasses M1 and is simulated internally. The somatosensory consequence of the simulated articulation is estimated over parietal somatosensory regions, including SI, SII, PO and SMG. The auditory consequences – in the form of an abstract auditory representation – is derived from the subsequent estimation. A highly specified auditory representation (thick arrow) is generated in the bottom-up perceptual process that goes through spectrotemporal analysis of external stimuli in STG (brown). The stream containing the motor simulation and perceptual estimation processes can enrich the specificity of predicted auditory representations (solid arrows), compared to enrichment from the memory-retrieval stream (dotted arrows). Abbreviations: STG, superior temporal gyrus; STS, superior temporal sulcus; pMTG, posterior middle temporal gyrus; MTL, middle temporal lobe; IFG, inferior frontal gyrus; PMC, premotor cortex; INS, insula; SMA, supplementary motor area; M1, primary motor cortex; PO, parietal operculum; SI, primary somatosensory cortex; SII, secondary somatosensory cortex; and SMG, supramarginal gyrus. 
 
 
 
 
 Fig. 2 
 
 Functional MRI data for the AI and HI tasks. Statistical parametric t-maps indicate strength of the BOLD signal ( p  < .001 uncorrected) with cluster size greater than 25 voxels. (a) Shared cortical regions that mediate both AI and HI. (b) Regions show greater activity in HI compared to that in AI, including MFG, IPC and IPS, consistent with frontal-parietal distributed memory retrieval networks. (c) Regions show greater activity in AI compared to that in HI, including SMC, PO, which demonstrate the greater recruitment of simulation-estimation stream in AI. Greater activity is also observed in aSTG and pSTS, suggesting that more specific and robust auditory representation is obtained during AI. Abbreviations: SMA, supplementary motor area; IFG, inferior frontal gyrus; INS, insula; FO, frontal operculum; PO, parietal operculum; PMC, premotor cortex; MFG, middle frontal gyrus; IPC, inferior parietal cortex; IPS, intraparietal sulcus; SMC, sensorimotor cortex; aSTG, anterior superior temporal gyrus; pSTS, posterior superior temporal sulcus. 
 
 
 
 
 Fig. 3 
 
 Functional MRI data for common activation in AI and A. Statistical parametric t-maps indicate strength of the BOLD signal ( p  < .001 uncorrected) with cluster size greater than 25 voxels. Abbreviations: SMA, supplementary motor area; IFG, inferior frontal gyrus; INS, insula; ACC, anterior cingulate cortex; GP, globus pallidus; PO, parietal operculum. 
 
 
 
 
 Table 1 
 
 Shared neural network for AI and HI, revealed by conjunction analysis. Multiple peaks in a cluster are presented, with local maxima at least 8 mm apart. The threshold is set at  t  > 3.65,  p  < .001 uncorrected, with an extent of at least 25 voxels. The clusters are presented in descending size order (by number of voxels). Shading represents different clusters, if multiple distinct regions are found in the same cluster. Abbreviations: BA, Brodmann area; SMA, supplementary motor area. 
 
 
 
 
 Brain region 
 # Voxels 
 t -Stat 
 z -Stat 
 MNI coordinates
 
 
 
 
 x 
 
 
 y 
 
 
 z 
 
 
 
 
 
 
 Frontal 
 
 
 
 L supplementary motor area (SMA) 
 1013 
 6.10 
 4.38 
 −6 
 −2 
 70 
 
 
 R pre-SMA 
 
 5.73 
 4.22 
 4 
 12 
 58 
 
 
 L inferior frontal pars opercularis (BA 44) 
 831 
 6.70 
 4.63 
 −58 
 6 
 12 
 
 
 L anterior insula 
 
 6.44 
 3.89 
 −36 
 10 
 6 
 
 
 R frontal operculum 
 380 
 5.99 
 4.33 
 48 
 8 
 6 
 
 
 R inferior frontal pars opercularis (BA 44) 
 
 4.48 
 3.59 
 54 
 14 
 4 
 
 
 L mid premotor cortex (BA 6) 
 146 
 5.92 
 4.30 
 −46 
 2 
 52 
 
 
 L primary motor cortex (near mouth region) 
 
 3.86 
 3.22 
 −42 
 −10 
 46 
 
 
 
 Parietal 
 
 
 
 L parietal operculum 
 50 
 5.38 
 4.06 
 −54 
 −42 
 28 
 
 
 
 Cerebellum/Subcortical 
 
 
 
 L cerebellum VI (declive) 
 30 
 4.81 
 3.77 
 −32 
 −60 
 −30 
 
 
 R cerebellum VI (declive) 
 45 
 4.66 
 3.69 
 32 
 −58 
 −30 
 
 
 
 
 
 Table 2 
 
 Regions of peak neural activity during HI compared with AI. Abbreviations: BA, Brodmann area; SMA, supplementary motor area. 
 
 
 
 
 Brain region 
 # Voxels 
 t -Stat 
 z -Stat 
 MNI coordinates
 
 
 
 
 x 
 
 
 y 
 
 
 z 
 
 
 
 
 
 
 Frontal 
 
 
 
 L middle frontal (BA 8) 
 121 
 5.26 
 4.00 
 −38 
 10 
 36 
 
 
 L inferior frontal pars opercularis (BA 44) 
 
 4.31 
 3.49 
 −44 
 4 
 26 
 
 
 R inferior frontal pars triangularis (BA 45) 
 54 
 5.50 
 4.11 
 46 
 32 
 14 
 
 
 L pre-SMA 
 33 
 5.34 
 4.04 
 −6 
 18 
 50 
 
 
 
 Parietal 
 
 
 
 L intraparietal sulcus 
 87 
 5.28 
 4.01 
 −42 
 −48 
 48 
 
 
 L inferior parietal gyrus 
 29 
 5.23 
 3.98 
 −28 
 −62 
 34 
 
 
 
 
 
 Table 3 
 
 Regions of peak neural activity during AI compared with HI. Abbreviations: BA, Brodmann area; aSTG, anterior superior temporal gyrus; pSTS, posterior temporal sulcus. 
 
 
 
 
 Brain region 
 # Voxels 
 t -Stat 
 z -Stat 
 MNI coordinates
 
 
 
 
 x 
 
 
 y 
 
 
 z 
 
 
 
 
 
 
 Frontal & Parietal 
 
 
 
 R superior frontal (BA 9) 
 60 
 4.87 
 3.8 
 18 
 60 
 26 
 
 
 R primary motor cortex 
 40 
 5.88 
 4.29 
 24 
 −20 
 60 
 
 
 L middle frontal (BA 46) 
 34 
 4.45 
 3.57 
 −32 
 48 
 26 
 
 
 L sensorimotor cortex 
 31 
 4.78 
 3.75 
 −54 
 −12 
 40 
 
 
 L subcentral gyrus (Rolandic operculum) 
 31 
 4.44 
 3.57 
 −52 
 −2 
 8 
 
 
 R sensorimotor cortex 
 25 
 4.65 
 3.68 
 56 
 0 
 18 
 
 
 L parietal operculum 
 78 
 5.96 
 4.32 
 −58 
 −28 
 22 
 
 
 
 Temporal/Insular 
 
 
 
 L aSTG 
 66 
 4.88 
 3.80 
 −54 
 6 
 −6 
 
 
 R pSTS 
 40 
 5.04 
 3.89 
 54 
 −26 
 2 
 
 
 L posterior insula 
 29 
 5.17 
 3.96 
 −38 
 −10 
 −12 
 
 
 
 Cerebellum/Subcortical 
 
 
 
 R caudate 
 41 
 5.28 
 4.01 
 20 
 22 
 4 
 
 
 
 
 
 Table 4 
 
 Shared neural network for A and AI, revealed by conjunction analysis. Abbreviations: BA, Brodmann area; SMA, supplementary motor area; ACC, anterior cingulate cortex. 
 
 
 
 
 Brain region 
 # Voxels 
 t -Stat 
 z -Stat 
 MNI coordinates
 
 
 
 
 x 
 
 
 y 
 
 
 z 
 
 
 
 
 
 
 Frontal/Insular 
 
 
 
 L mid-dorsal insula 
 679 
 7.12 
 4.78 
 −34 
 8 
 10 
 
 
 L inferior frontal pars opercularis (BA 44) 
 
 5.84 
 4.27 
 −60 
 8 
 14 
 
 
 L SMA 
 584 
 6.03 
 4.35 
 −6 
 −2 
 72 
 
 
 R pre-SMA 
 
 5.71 
 4.21 
 6 
 4 
 68 
 
 
 R frontal operculum 
 425 
 6.36 
 4.49 
 46 
 6 
 6 
 
 
 R inferior frontal pars opercularis (BA 44) 
 
 5.18 
 3.96 
 56 
 14 
 2 
 
 
 R ACC (BA 32) 
 222 
 4.71 
 3.72 
 2 
 22 
 34 
 
 
 L ACC (BA 32) 
 
 4.34 
 3.51 
 −6 
 28 
 24 
 
 
 
 Parietal 
 
 
 
 L parietal operculum 
 33 
 4.52 
 3.62 
 −52 
 −40 
 28 
 
 
 
 Cerebellum/Subcortical 
 
 
 
 L putamen 
 70 
 4.94 
 3.84 
 −28 
 −8 
 14 
 
 
 L globus pallidus 
 66 
 6.55 
 4.57 
 −16 
 −12 
 0 
 
 
 R globus pallidus 
 45 
 5.25 
 4.00 
 14 
 0 
 −6 
 
 
 R caudate 
 
 3.81 
 3.20 
 8 
 6 
 4 
 
 
 R cerebellum VI (declive) 
 41 
 4.44 
 3.57 
 34 
 −60 
 −28 
 
 
 L cerebellum VI (declive) 
 32 
 4.81 
 3.77 
 −32 
 −60 
 −30 
 
 
 
 
 
